{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpmhxpjtKlYUrdbeuftkYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/segmue/GIR_Project/blob/main/Irchel_Geoparser_ACLED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "# Path to the folder containing the CSV files\n",
        "data_folder = '/content/drive/MyDrive/GEO871'  # Replace with the actual path\n",
        "\n",
        "def load_files(data_folder):\n",
        "  dataframes = {}\n",
        "  for filename in os.listdir(data_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "          filepath = os.path.join(data_folder, filename)\n",
        "          try:\n",
        "              df = pd.read_csv(filepath)\n",
        "              dataframes[filename] = df\n",
        "              print(f\"Successfully loaded '{filename}' into a DataFrame.\")\n",
        "          except Exception as e:\n",
        "              print(f\"Error loading '{filename}': {e}\")\n",
        "  return dataframes\n",
        "\n",
        "\n",
        "def split_batches(df, batchsize):\n",
        "  batches = {}\n",
        "  num_batches = (len(df) + batchsize - 1) // batchsize  # Calculate the number of batches\n",
        "  for i in range(num_batches):\n",
        "      start_index = i * batchsize\n",
        "      end_index = min((i + 1) * batchsize, len(df))\n",
        "      batches[i] = df.iloc[start_index:end_index]\n",
        "  return batches\n",
        "\n",
        "def write_df_to_drive(df, filename):\n",
        "  \"\"\"Writes a DataFrame to a pickle file in the Google Drive data folder.\n",
        "\n",
        "  Args:\n",
        "    df: The DataFrame to write.\n",
        "    filename: The name of the pickle file (including the .pkl extension).\n",
        "  \"\"\"\n",
        "  filepath = os.path.join(data_folder, filename)\n",
        "  try:\n",
        "    with open(filepath, 'wb') as f:\n",
        "      pickle.dump(df, f)\n",
        "    print(f\"DataFrame successfully written to '{filepath}'\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error writing DataFrame to '{filepath}': {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcMRfNU7VeeN",
        "outputId": "1e12a217-26a6-4d60-a841-c5edf48427d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmu1XxVPJ3qz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install geoparser==0.1.8\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m geoparser download geonames"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from geoparser import Geoparser\n",
        "geo = Geoparser(spacy_model='en_core_web_sm', transformer_model='dguzh/geo-all-MiniLM-L6-v2', gazetteer='geonames')\n",
        "\n",
        "def geoparse_df_notes(df):\n",
        "    # Apply geo.parse to the entire \"notes\" column at once\n",
        "    parsed_rows = geo.parse(df[\"notes\"].tolist())\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    toponyms_col = []\n",
        "    locations_col = []\n",
        "    res_list_col = []\n",
        "\n",
        "    # Process each parsed_row\n",
        "    for parsed_row in parsed_rows:\n",
        "        toponyms = []\n",
        "        locations = []\n",
        "        res_list = []\n",
        "\n",
        "        for t in parsed_row.toponyms:\n",
        "            toponyms.append(str(t))\n",
        "            t_loc = t.location\n",
        "            locations.append(t_loc)\n",
        "\n",
        "            try:\n",
        "                t_lat = t_loc.get(\"latitude\", None) if t_loc else None\n",
        "                t_long = t_loc.get(\"longitude\", None) if t_loc else None\n",
        "            except AttributeError:\n",
        "                t_lat = None\n",
        "                t_long = None\n",
        "\n",
        "            subres = {str(t): {\"latitude\": t_lat, \"longitude\": t_long}}\n",
        "            res_list.append(subres)\n",
        "\n",
        "        # Append results for this row\n",
        "        toponyms_col.append(toponyms)\n",
        "        locations_col.append(locations)\n",
        "        res_list_col.append(res_list)\n",
        "\n",
        "    # Add results to the DataFrame\n",
        "    df[\"geoparser_locations\"] = locations_col\n",
        "    df[\"geoparser_toponyms\"] = toponyms_col\n",
        "    df[\"geoparser_res\"] = res_list_col\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dz2MRnfsKI1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_files(data_folder)\n"
      ],
      "metadata": {
        "id": "eY_B82DNWTl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, df in data.items():\n",
        "  batches = split_batches(df, 10000)\n",
        "  data[filename] = batches"
      ],
      "metadata": {
        "id": "lkZ6OEGvfnCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename, df in data.items():\n",
        "  for batch_num, batch_df in df.items():\n",
        "    if filename == 'Europe-Central-Asia_2018-2024_Nov22.csv': # and int(batch_num) <= 30:\n",
        "      print(f\"Skipping '{filename}' batch {batch_num}\")\n",
        "      continue\n",
        "    result = geoparse_df_notes(batch_df)\n",
        "    write_filename = filename[:-4] + \"_\" + str(batch_num) + \".pkl\"\n",
        "    write_df_to_drive(result, write_filename)\n",
        "    print(f\"Successfully wrote '{write_filename}'\")\n"
      ],
      "metadata": {
        "id": "ZQ37Tj5nLb0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}